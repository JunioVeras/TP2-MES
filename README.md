# TP2-MES â€“ AnÃ¡lise de CorreÃ§Ã£o de Bugs com Modelos de Linguagem

## ğŸ§  Proposta do Trabalho

Este trabalho tem como objetivo avaliar a capacidade de modelos de linguagem natural (LLMs) em corrigir bugs simples em cÃ³digo-fonte. Para isso, utilizaremos um dataset com exemplos de bugs de uma linha (one-line bugs) e analisaremos a eficÃ¡cia das correÃ§Ãµes propostas pela LLM, tanto quantitativamente (execuÃ§Ã£o de testes) quanto qualitativamente (anÃ¡lise das mudanÃ§as).

## ğŸ‘¥ Membros

- JÃºnio Veras de Jesus Lima (SI)
- Ester Sara Assis (CC)
- Filipe Pirola (CC)
- Igor Eduardo Braga (CC)

## ğŸ” Metodologia

### ğŸ¤– Modelo de Linguagem

Neste projeto, utilizaremos dois modelos de linguagem open source: LLaMA e DeepSeek, ambos amplamente adotados na comunidade de inteligÃªncia artificial e disponÃ­veis na plataforma Hugging Face. A Hugging Face Ã© uma plataforma amplamente usada na Ã¡rea de inteligÃªncia artificial para disponibilizar e compartilhar modelos de machine learning de forma acessÃ­vel e padronizada.

### ğŸ—‚ï¸ Dataset

SerÃ¡ utilizado o dataset [**QuixBugs**](https://github.com/jkoppel/QuixBugs), que contÃ©m 40 pequenos programas com bugs, implementados em Java e Python. Cada programa possui:

- Uma versÃ£o com bug (*buggy*).
- Uma versÃ£o corrigida (*corrected*).
- Um conjunto de testes automatizados para validaÃ§Ã£o da correÃ§Ã£o.

O foco principal do dataset sÃ£o bugs localizados em poucas linhas, o que facilita o uso com modelos de linguagem. Caso necessÃ¡rio, o conjunto poderÃ¡ ser expandido via mutaÃ§Ã£o ou geraÃ§Ã£o de novos exemplos. 

### âœï¸ Exemplos Preliminares de Prompts

Exemplo de primeiro prompt ingÃªnuo, definido no padrÃ£o comum de LLMs com modelos disponÃ­veis no HuggingFace. Pode ter seu formato alterado se o modelo requerir. 

````
[
  {
    "role": "system",
    "content": "You are a helpful AI programming assistant. When the user sends you a piece of code that contains a bug, your job is to return the corrected version of the code. "
  },
  {
    "role": "user",
    "content": "def add_numbers(a, b):\n    return a + b\n\nprint(add_numbers(2))"
  }
]
````

### ğŸ“ AvaliaÃ§Ã£o Quantitativa

A avaliaÃ§Ã£o quantitativa serÃ¡ feita a partir da **execuÃ§Ã£o dos testes automatizados** incluÃ­dos no QuixBugs. Para cada cÃ³digo corrigido pela LLM, serÃ¡ verificada a proporÃ§Ã£o de correÃ§Ãµes que passaram todos os testes.

### ğŸ§ª AvaliaÃ§Ã£o Qualitativa

A avaliaÃ§Ã£o qualitativa serÃ¡ realizada manualmente, inspecionando exemplos de correÃ§Ãµes:
- Que foram bem-sucedidas mas diferentes da correÃ§Ã£o original.
- Que passaram nos testes, mas possuem problemas de estilo ou boas prÃ¡ticas.

Essa anÃ¡lise ajudarÃ¡ a compreender as limitaÃ§Ãµes e o comportamento do modelo frente a diferentes tipos de bugs.

---

## ğŸ“ ReferÃªncias

- [QuixBugs Dataset](https://github.com/jkoppel/QuixBugs)
- [HuggingFace](https://huggingface.co/)
